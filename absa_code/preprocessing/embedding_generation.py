import numpy as np

GLOVE_ROOTPATH = '../embedding_data/'
GLOVE_DEFAULT_FILENAME = 'glove.6B.{}d.txt'

def build_pretrained_embedding(f):
    ''' 
    Builds pretrained embedding dictionary from inputed GloVe file \\
    Returns a dictionary (`key`: word, `value`: word vector)
    '''
    embeddings_index = {}
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs
    return embeddings_index

def generate_ngram_matrix(texts, emb_dim=100, glove_path=GLOVE_ROOTPATH, glove_filename=GLOVE_DEFAULT_FILENAME):
    '''
    Builds a 2D matrix representation of the inputed `texts`
    using pretrained GloVe word embeddings.

    `emb_dim` must be 50, 100, 200, or 300

    Returns a numpy matrix with shape (?, emb_dim)
    '''
    if emb_dim not in {50, 100, 200, 300}: raise ValueError('emb_dim must be 50, 100, 200, or 300')
    f = open(glove_path + glove_filename.format(str(emb_dim)))
    emb_dict = build_pretrained_embedding(f)

    line_vecs = []
    for line in texts:
        vecs = []
        words = line.split(' ')
        for word in words:
            if word not in emb_dict:
                vec = np.zeros(emb_dim)
            else:
                vec = emb_dict[word]
            vecs.append(vec)

        line_vec = np.stack(vecs)
        line_vecs.append(line_vec)
    
    return np.stack(line_vecs)

def flatten_sentence_vectors(word_matrix):
    '''
    Returns a flattened 1D vector per sentence, with 
    length equal to number of words in the sentence and
    the embedding dimension.

    Requires a word matrix, like the one generated by
    `generate_glove_word_vectors()`, as input.

    This function WILL NOT look at the pretrained 
    embeddings itself, it simply flattens the matrix
    representation.

    in_shape: (?, x, y)
    out_shape: (?, x*y)
    '''
    new_vecs = []
    for sent_vec in word_matrix:
        r, c = sent_vec.shape
        new_vec = sent_vec.reshape((r*c))
        new_vecs.append(new_vec)
    
    return np.stack(new_vecs)

def check_embedding(text, text_embedding, emb_dict):
    '''
    Simple check to make sure an inputted sentence
    or text fragment correctly matches a matrix
    of word vectors.

    `text`: text fragment (str) \\
    `text_embedding`: appended word vectors to check \\
    `emb_dict`: dictionary containing embeddings
    '''
    for word, word_emb in zip(text.split(), text_embedding):
        if (emb_dict[word] != word_emb).all(): return False
    return True